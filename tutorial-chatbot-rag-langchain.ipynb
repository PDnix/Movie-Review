{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -q langchain langchain-community langchain_huggingface chromadb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:06:09.072497Z","iopub.execute_input":"2025-01-23T06:06:09.073152Z","iopub.status.idle":"2025-01-23T06:06:18.671000Z","shell.execute_reply.started":"2025-01-23T06:06:09.073119Z","shell.execute_reply":"2025-01-23T06:06:18.669976Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 1. Use langchain RAG","metadata":{}},{"cell_type":"code","source":"import os\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:06:50.878487Z","iopub.execute_input":"2025-01-23T06:06:50.879306Z","iopub.status.idle":"2025-01-23T06:06:52.019065Z","shell.execute_reply.started":"2025-01-23T06:06:50.879269Z","shell.execute_reply":"2025-01-23T06:06:52.018443Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"your_API_TOKEN\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:21:31.599174Z","iopub.execute_input":"2025-01-23T06:21:31.599527Z","iopub.status.idle":"2025-01-23T06:21:31.603658Z","shell.execute_reply.started":"2025-01-23T06:21:31.599497Z","shell.execute_reply":"2025-01-23T06:21:31.602709Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:32:24.841782Z","iopub.execute_input":"2024-11-06T03:32:24.842623Z","iopub.status.idle":"2024-11-06T03:32:24.855683Z","shell.execute_reply.started":"2024-11-06T03:32:24.84255Z","shell.execute_reply":"2024-11-06T03:32:24.85326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.llms import HuggingFaceHub\n\n# set Korean embedding and llm odel\nhf_embeddings = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n\nhf_llm = HuggingFaceHub(\n    repo_id=\"skt/kogpt2-base-v2\",\n    model_kwargs={\"task\": \"text-generation\"} ## question-answering tasK X. text-generation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:24:45.712364Z","iopub.execute_input":"2025-01-23T06:24:45.713275Z","iopub.status.idle":"2025-01-23T06:25:07.742134Z","shell.execute_reply.started":"2025-01-23T06:24:45.713240Z","shell.execute_reply":"2025-01-23T06:25:07.741174Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3316765938.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  hf_embeddings = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc69f577cc5740abbff871a2b51a5063"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86be17b03c2d465a8331629e95a21c8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.86k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e0c7ed11dc64b18ae1f7ed7c4dff54e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c36024a89a4342c0b88660a528d24a2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/744 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9caeb3e63f243ad94a90417cf73c806"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9efe26975ee248c58c5849ee78770026"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29c6a468bbbd4348b8e4376a6bf895c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d979dd81c77a4ff3a7577684c6a5e729"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"128c6442888349a0a0e7871530041f10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a19539e12baf4278b29c2d6621bf05dc"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7d0dc18d937477cbd0f90cf4762cd00"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/3316765938.py:7: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n  hf_llm = HuggingFaceHub(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import requests\nfrom langchain.schema import Document\nfrom bs4 import BeautifulSoup\n\n# for Wikipedia documents (EN, KO)\n\n# from langchain_community.document_loaders import WikipediaLoader\n\n# By default, English documents (https://en.wikipedia.org))\n# def load_Wiki_docs(query):\n#     loader = WikipediaLoader(query=query, load_max_docs=1) # need !pip install wikipedia\n#     documents = loader.load()\n    \n#     text_splitter = RecursiveCharacterTextSplitter(\n#         chunk_size=1000,\n#         chunk_overlap=200\n#     )\n#     splits = text_splitter.split_documents(documents)\n    \n#     return splits\n\n\n# For Korean query, get results from Korean wikipedia website and crawl and parse results\ndef load_Korean_wiki_docs(topic):\n    url = f\"https://ko.wikipedia.org/wiki/{topic}\"\n    \n    response = requests.get(url)\n    response.raise_for_status()  # raise Exception when error occurs\n\n    # HTML parsing and extract body contents\n    soup = BeautifulSoup(response.text, 'html.parser')\n    content = soup.find('div', {'class': 'mw-parser-output'})  # find div including body contents \n    \n    # Extract contents\n    paragraphs = content.find_all('p')\n    text = \"\\n\".join([p.get_text() for p in paragraphs])  # concat all context in <p> tags \n \n    # convert to Document object (required for LangChain)\n    documents = [Document(page_content=text, metadata={\"source\": url})]\n    \n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200\n    )\n    splits = text_splitter.split_documents(documents)\n    \n    return splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:25:24.696290Z","iopub.execute_input":"2025-01-23T06:25:24.696950Z","iopub.status.idle":"2025-01-23T06:25:24.828595Z","shell.execute_reply.started":"2025-01-23T06:25:24.696914Z","shell.execute_reply":"2025-01-23T06:25:24.827929Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def create_vectorstore(splits): \n    vectorstore = Chroma.from_documents(documents=splits, embedding=hf_embeddings)\n    return vectorstore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:26:00.871899Z","iopub.execute_input":"2025-01-23T06:26:00.873080Z","iopub.status.idle":"2025-01-23T06:26:00.877093Z","shell.execute_reply.started":"2025-01-23T06:26:00.873046Z","shell.execute_reply":"2025-01-23T06:26:00.875894Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"topic = \"흑백요리사\"\n# Load wikipedia documents for this topic\nsplits = load_Korean_wiki_docs(topic) \n# Create vectorstore with this fetched docs\nvectorstore = create_vectorstore(splits)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:26:05.822842Z","iopub.execute_input":"2025-01-23T06:26:05.823171Z","iopub.status.idle":"2025-01-23T06:26:07.235976Z","shell.execute_reply.started":"2025-01-23T06:26:05.823142Z","shell.execute_reply":"2025-01-23T06:26:07.235276Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def create_rag_chain(vectorstore):\n    prompt_template = \"\"\"문맥을 참고하여 질문에 정확하고 간결하게 답하십시오.\n    문맥: {context}\n    질문: {question}\n    답변:\"\"\"\n    \n    PROMPT = PromptTemplate(\n        template=prompt_template, input_variables=[\"context\", \"question\"]\n    )\n\n    chain_type_kwargs = {\"prompt\": PROMPT}\n\n    # Make context shorter\n    # def short_context(context, max_length=300):\n    #     return context[:max_length] if len(context) > max_length else context\n    \n    # class ShortContextRetriever(BaseRetriever):\n    #     def __init__(self, retriever):\n    #         super().__init__()\n    #         self._retriever = retriever\n        \n    #     def get_relevant_documents(self, query):\n    #         docs = self._retriever.get_relevant_documents(query)\n    #         for doc in docs:\n    #             doc.page_content = short_context(doc.page_content)\n    #         return docs\n    \n    # retriever = ShortContextRetriever(vectorstore.as_retriever())\n\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=hf_llm,\n        chain_type=\"stuff\",\n        retriever=vectorstore.as_retriever(),\n        chain_type_kwargs=chain_type_kwargs,\n        return_source_documents=True\n    )\n    \n    return qa_chain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:28:09.418075Z","iopub.execute_input":"2025-01-23T06:28:09.418636Z","iopub.status.idle":"2025-01-23T06:28:09.424123Z","shell.execute_reply.started":"2025-01-23T06:28:09.418604Z","shell.execute_reply":"2025-01-23T06:28:09.423256Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# create langchang RAG chain\nqa_chain = create_rag_chain(vectorstore)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:33:58.917251Z","iopub.execute_input":"2025-01-23T06:33:58.917626Z","iopub.status.idle":"2025-01-23T06:33:58.922537Z","shell.execute_reply.started":"2025-01-23T06:33:58.917594Z","shell.execute_reply":"2025-01-23T06:33:58.921591Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"question = \"심사위원을 누가 맡았어?\"\n\n# result = qa_chain({\"query\": question})\nresult = qa_chain.invoke({\"query\": question})\n\nprint (\"결과:\")\nprint(result[\"result\"])\n\nprint(\"출처:\")\nfor doc in result[\"source_documents\"]:\n    print(doc.page_content)\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:34:05.372341Z","iopub.execute_input":"2025-01-23T06:34:05.373209Z","iopub.status.idle":"2025-01-23T06:34:59.308555Z","shell.execute_reply.started":"2025-01-23T06:34:05.373174Z","shell.execute_reply":"2025-01-23T06:34:59.307699Z"}},"outputs":[{"name":"stdout","text":"결과:\n문맥을 참고하여 질문에 정확하고 간결하게 답하십시오.\n    문맥: 《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]\n    질문: 심사위원을 누가 맡았어?\n    답변: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원:\n출처:\n《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]\n---\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"docs = vectorstore.as_retriever().get_relevant_documents(question)\ndocs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:35:20.970287Z","iopub.execute_input":"2025-01-23T06:35:20.970952Z","iopub.status.idle":"2025-01-23T06:35:20.995091Z","shell.execute_reply.started":"2025-01-23T06:35:20.970918Z","shell.execute_reply":"2025-01-23T06:35:20.994280Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2742417880.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  docs = vectorstore.as_retriever().get_relevant_documents(question)\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source': 'https://ko.wikipedia.org/wiki/흑백요리사'}, page_content='《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]')]"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"docs = vectorstore.similarity_search(question, k=4)\ndocs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:35:26.905226Z","iopub.execute_input":"2025-01-23T06:35:26.905852Z","iopub.status.idle":"2025-01-23T06:35:26.928379Z","shell.execute_reply.started":"2025-01-23T06:35:26.905817Z","shell.execute_reply":"2025-01-23T06:35:26.927637Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source': 'https://ko.wikipedia.org/wiki/흑백요리사'}, page_content='《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]')]"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# It seems vectorDB loading from embedding model works fine, but seems llm model does not.\n# Some Korean llm model seems to work fine in text-generation task, but for Question-Ansering task, we might need another approach.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:15.580119Z","iopub.execute_input":"2024-11-06T03:33:15.580617Z","iopub.status.idle":"2024-11-06T03:33:15.586739Z","shell.execute_reply.started":"2024-11-06T03:33:15.580573Z","shell.execute_reply":"2024-11-06T03:33:15.585086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Use QA pipeline with vectorstor similarity search","metadata":{}},{"cell_type":"code","source":"# import torch\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\n# Load model and tokenizer\nmodel_name = \"yjgwak/klue-bert-base-finetuned-squard-kor-v1\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Set Q_A pipeline\nqa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:39:24.802302Z","iopub.execute_input":"2025-01-23T06:39:24.803141Z","iopub.status.idle":"2025-01-23T06:39:36.153341Z","shell.execute_reply.started":"2025-01-23T06:39:24.803106Z","shell.execute_reply":"2025-01-23T06:39:36.152456Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/635 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22343f4235ef4f48a7b5d8b7512cde63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42e99a74389f4592a2f193f0820a1ad5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14d75e1dfba94b7c82894296740a78c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/246k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"798c56bf558d489a983bbd99d2093c4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc166c4ea16d418392dbb7230641e235"}},"metadata":{}},{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Example: define question and context \nquestion = \"오늘 날씨 어때?\"\ncontext = \"오늘의 날씨는 맑고 따뜻한 기온이 유지될 것으로 보입니다.\"\n\n# model chain\nresult = qa_pipeline(question=question, context=context)\n\n# Result\nprint(\"질문:\", question)\nprint(\"답변:\", result['answer'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:31.423141Z","iopub.execute_input":"2024-11-06T03:33:31.423617Z","iopub.status.idle":"2024-11-06T03:33:31.555002Z","shell.execute_reply.started":"2024-11-06T03:33:31.423565Z","shell.execute_reply":"2024-11-06T03:33:31.553646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# search context in VectorStore\ndef retrieve_context(question, vectorstore):\n    docs = vectorstore.similarity_search(question, k=4)\n    if docs:\n        return \" \".join([doc.page_content for doc in docs])\n        # return docs[0].page_content  # return first relevant doc\n    else:\n        return None\n\n# Generate answer based on query and searched context similar to RAG chain\ndef answer_question_with_context(question, vectorstore):\n    context = retrieve_context(question, vectorstore)\n    if context:\n        result = qa_pipeline(question=question, context=context)\n        return result['answer'], context  # return answer and used source doc\n    else:\n        return \"관련 문맥을 찾지 못했습니다.\", None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:31.556691Z","iopub.execute_input":"2024-11-06T03:33:31.557138Z","iopub.status.idle":"2024-11-06T03:33:31.566617Z","shell.execute_reply.started":"2024-11-06T03:33:31.557096Z","shell.execute_reply":"2024-11-06T03:33:31.565007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example\nquestion = \"심사위원을 누가 맡았어?\"\n\nanswer, used_context = answer_question_with_context(question, vectorstore)\n\nprint(\"질문:\", question)\nprint(\"답변:\", answer)\nprint(\"사용된 문맥:\", used_context)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:31.56847Z","iopub.execute_input":"2024-11-06T03:33:31.568961Z","iopub.status.idle":"2024-11-06T03:33:31.902837Z","shell.execute_reply.started":"2024-11-06T03:33:31.568917Z","shell.execute_reply":"2024-11-06T03:33:31.901412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Use Gemini+RAG","metadata":{}},{"cell_type":"code","source":"# It seems the best and simple and cost-free option when OpenAI api cannot be used.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:31.904985Z","iopub.execute_input":"2024-11-06T03:33:31.905918Z","iopub.status.idle":"2024-11-06T03:33:31.912362Z","shell.execute_reply.started":"2024-11-06T03:33:31.905848Z","shell.execute_reply":"2024-11-06T03:33:31.910324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -q langchain langchain-community langchain_huggingface chromadb google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:56:09.430891Z","iopub.execute_input":"2025-01-23T06:56:09.431304Z","iopub.status.idle":"2025-01-23T06:56:19.305602Z","shell.execute_reply.started":"2025-01-23T06:56:09.431271Z","shell.execute_reply":"2025-01-23T06:56:19.304411Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# pip install google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:52.362965Z","iopub.execute_input":"2024-11-06T03:33:52.363439Z","iopub.status.idle":"2024-11-06T03:33:52.369738Z","shell.execute_reply.started":"2024-11-06T03:33:52.363381Z","shell.execute_reply":"2024-11-06T03:33:52.368235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.schema import Document\nfrom langchain.llms import OpenAI\nimport google.generativeai as genai\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:56:36.533922Z","iopub.execute_input":"2025-01-23T06:56:36.534658Z","iopub.status.idle":"2025-01-23T06:56:36.867732Z","shell.execute_reply.started":"2025-01-23T06:56:36.534623Z","shell.execute_reply":"2025-01-23T06:56:36.867023Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR-API-KEY\"\ngenai_api_key = \"your_api_key\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:01:36.522360Z","iopub.execute_input":"2025-01-23T07:01:36.523129Z","iopub.status.idle":"2025-01-23T07:01:36.526931Z","shell.execute_reply.started":"2025-01-23T07:01:36.523096Z","shell.execute_reply":"2025-01-23T07:01:36.525967Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"genai.configure(api_key=genai_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:01:45.163817Z","iopub.execute_input":"2025-01-23T07:01:45.164490Z","iopub.status.idle":"2025-01-23T07:01:45.168506Z","shell.execute_reply.started":"2025-01-23T07:01:45.164444Z","shell.execute_reply":"2025-01-23T07:01:45.167580Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# 1. Gemini model\ngemini_model = genai.GenerativeModel('gemini-1.5-flash')\n\n# 2. embedding model\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:01:50.261781Z","iopub.execute_input":"2025-01-23T07:01:50.262110Z","iopub.status.idle":"2025-01-23T07:01:55.521814Z","shell.execute_reply.started":"2025-01-23T07:01:50.262081Z","shell.execute_reply":"2025-01-23T07:01:55.521067Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62eb7c6e6e9344fdb581ddd167e82c75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1bb3786ebc84399a19721ab53964ddb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"835d19b29efd458c832c648aca072283"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9640522f06204201bcbcdeca9e8d0d06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"716a5960e5e34839b4be841478c7bd89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"385259fbc15d41e5822bfa861f0f97be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56ebce577de14ef39ce869d3396f4e15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b82571f3faaf405d8e82799562fedd88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99ee4ad074714b54b87d575f75337fdf"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30ed76257ecb410da8c4965fcdbbd27b"}},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"from langchain.vectorstores import Chroma\n# sample docs\ndocs = [\n    Document(page_content=\"한국어 챗봇은 자연어 처리 기술을 사용하여 사용자와 대화를 나눕니다.\", metadata={\"source\": \"doc1\"}),\n    Document(page_content=\"인공지능을 활용한 챗봇은 여러 산업에서 사용되고 있습니다.\", metadata={\"source\": \"doc2\"}),\n    Document(page_content=\"한국어와 영어를 동시에 지원하는 챗봇이 점점 늘어나고 있습니다.\", metadata={\"source\": \"doc3\"}),\n    Document(page_content=\"챗봇은 고객 서비스를 개선하고 사용자 경험을 향상시키는 데 중요한 역할을 합니다.\", metadata={\"source\": \"doc4\"})\n]\n\n# to avoid collision with previous one\npersist_directory = \"./new_chroma_db\"\n\nvectorstore = Chroma.from_documents(docs, embedding=embedding_model, persist_directory=\"./chroma_db\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:33:19.457792Z","iopub.execute_input":"2025-01-23T07:33:19.458662Z","iopub.status.idle":"2025-01-23T07:33:19.518744Z","shell.execute_reply.started":"2025-01-23T07:33:19.458621Z","shell.execute_reply":"2025-01-23T07:33:19.518062Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# RAG using prompt\ndef rag_chatbot(question):\n    context_doc = vectorstore.similarity_search(question, k=1)\n    # context = context_doc[0].page_content if context_doc else \"정보를 찾을 수 없습니다.\"\n\n    context = \" \".join([doc.page_content for doc in context_doc])\n    \n    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer in a complete sentence:\"\n    # response = gemini_model(prompt)\n    \n    response = gemini_model.generate_content(prompt)\n    answer = response.candidates[0].content.parts[0].text\n\n    # print(\"출처 문서:\", context)\n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:40:35.531758Z","iopub.execute_input":"2025-01-23T07:40:35.532507Z","iopub.status.idle":"2025-01-23T07:40:35.537422Z","shell.execute_reply.started":"2025-01-23T07:40:35.532472Z","shell.execute_reply":"2025-01-23T07:40:35.536580Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# sample question\nquestion = \"챗봇이 어떤 기술을 사용하나요?\"\nresponse = rag_chatbot(question)\n\nprint(\"질문:\", question)\nprint(\"답변:\", response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:40:38.846624Z","iopub.execute_input":"2025-01-23T07:40:38.847299Z","iopub.status.idle":"2025-01-23T07:40:39.511957Z","shell.execute_reply.started":"2025-01-23T07:40:38.847267Z","shell.execute_reply":"2025-01-23T07:40:39.511124Z"}},"outputs":[{"name":"stdout","text":"질문: 챗봇이 어떤 기술을 사용하나요?\n답변: 챗봇은 자연어 처리(NLP), 기계 학습(ML), 심층 학습(DL)과 같은 인공지능 기술을 사용합니다.\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"prompt = f\"질문: 김이나 작사가의 작사 특이점으로 적금을 주제로 작사해 줄래? \\n대답: \"\nresponse = gemini_model.generate_content(prompt)\n    \nanswer = response.candidates[0].content.parts[0].text\nprint(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:20:43.193594Z","iopub.execute_input":"2025-01-23T07:20:43.194614Z","iopub.status.idle":"2025-01-23T07:20:49.745164Z","shell.execute_reply.started":"2025-01-23T07:20:43.194565Z","shell.execute_reply":"2025-01-23T07:20:49.744243Z"}},"outputs":[{"name":"stdout","text":"## 콩나물 시루 같은 내 적금통장 (김이나풍 가사)\n\n(Verse 1)\n월급날 딱 떨어진 알림에 심장이 콩닥\n세상 즐거운 소리, 삑! 하고 울리는 경고음 같아\n오늘도 난 짠돌이 모드, 텅장의 늪에서 벗어나\n저축 버튼 누르는 손길, 왠지 모르게 뿌듯해\n\n(Chorus)\n콩나물 시루 같은 내 적금통장, 콩알만 한 희망이 자라나\n매달 조금씩 불어나는 숫자, 작지만 확실한 행복의 증거\n미래의 나를 위한 선물, 꿈을 향한 작은 발걸음\n언젠가 펼쳐질 화려한 날들을 위해, 오늘도 난 저축해\n\n(Verse 2)\n치킨 한 마리 포기, 쇼핑 욕구 참아내는 연습\n마음은 벌써 백화점, 현실은 적금 이자 계산\n친구들 파티 초대, 괜찮아, 난 내일의 나를 위해\n오늘 밤은 잔잔한 음악과 함께, 미래를 꿈꿔봐\n\n(Chorus)\n콩나물 시루 같은 내 적금통장, 콩알만 한 희망이 자라나\n매달 조금씩 불어나는 숫자, 작지만 확실한 행복의 증거\n미래의 나를 위한 선물, 꿈을 향한 작은 발걸음\n언젠가 펼쳐질 화려한 날들을 위해, 오늘도 난 저축해\n\n(Bridge)\n어쩌면 늦은 건 아닐까, 망설이고 고민했던 시간들\n하지만 시작이 반이라 했지, 작은 씨앗 하나 심어놓고\n묵묵히 기다리는 마음, 그 안에 꿈틀거리는 설렘\n이 작은 콩알들이 언젠가 푸른 잎을 낼 거야\n\n(Chorus)\n콩나물 시루 같은 내 적금통장, 콩알만 한 희망이 자라나\n매달 조금씩 불어나는 숫자, 작지만 확실한 행복의 증거\n미래의 나를 위한 선물, 꿈을 향한 작은 발걸음\n언젠가 펼쳐질 화려한 날들을 위해, 오늘도 난 저축해\n\n(Outro)\n콩나물 시루…  내 통장…  (작은 목소리로)  쑥쑥 자라라…\n\n\n**김이나 작사가 특징 반영:**\n\n* **일상적인 소재와 감정을 활용:**  적금이라는 평범한 소재를  '콩나물 시루' 와 같은 친근한 비유로 표현.\n* **솔직하고 공감가는 가사:**  저축의 어려움과 뿌듯함을 현실적으로 표현.  ('치킨 한 마리 포기', '쇼핑 욕구 참아내는 연습' 등)\n* **반복되는 후렴구와 간결한 표현:**  중독성 있는 멜로디를 만들어낼 수 있도록 반복과 간결함을 추구.\n* **상징적인 표현:** '콩알만 한 희망', '푸른 잎' 등 시각적이고 감각적인 이미지를 활용하여  적금의 성장을 표현.\n* **마지막 여운:**  '쑥쑥 자라라' 와 같은 속삭이는 듯한 마무리로  듣는 이에게 따뜻한 느낌을 남김.\n\n\n이 가사는 김이나 작사가의 스타일을 참고하여 작성되었지만, 실제 김이나 작사가의 작품이 아니라는 점을 알려드립니다.\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}